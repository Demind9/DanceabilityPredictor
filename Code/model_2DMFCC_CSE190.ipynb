{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "338JJSokbDMo",
        "outputId": "0b5172d0-3ccd-4695-cde9-57cafeb957f3"
      },
      "id": "338JJSokbDMo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls 'drive/MyDrive/Spotify feature classification/Code/Data and Weights'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5xJGgQCbQio",
        "outputId": "c3052954-ed45-4143-e5ef-7bbfc8eaeaca"
      },
      "id": "f5xJGgQCbQio",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2DSpec_wBest1.hdf5\t        CSE190_proj_data.ipynb\n",
            " 2DSpec_wBest.hdf5\t       'CSE190_spectrogram data.ipynb'\n",
            " 2DSpec_wCurrent1.h5\t        JaarSongs\n",
            " 2DSpec_wCurrent.h5\t        L_array_44100.pkl\n",
            " 2DSpec_wCurrent_noDropout.h5   mono_mfcc_normedz_44100.pkl\n",
            " 3DSpec_wBest.hdf5\t        MS_mfcc_normedz_44100.pkl\n",
            " 3DSpec_wCurrent1.h5\t        X_array_22050.pkl\n",
            " 3DSpec_wCurrent.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "file_path = 'drive/MyDrive/Spotify feature classification/Code/Data and Weights/MS_mfcc_normedz_44100.pkl'\n",
        "destination_path = '../content/sample_data/'\n",
        "shutil.copyfile(file_path, destination_path + 'MS_mfcc_normedz_44100.pkl')\n",
        "\n",
        "file_path = 'drive/MyDrive/Spotify feature classification/Code/Data and Weights/L_array_44100.pkl'\n",
        "destination_path = '../content/sample_data/'\n",
        "shutil.copyfile(file_path, destination_path + 'L_array_44100.pkl')"
      ],
      "metadata": {
        "id": "UHbo3FFKbEvt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23a12018-567e-49d2-ddac-2d2dccbffc2c"
      },
      "id": "UHbo3FFKbEvt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'../content/sample_data/L_array_44100.pkl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "573c14bd",
      "metadata": {
        "id": "573c14bd",
        "outputId": "4e80c0b7-383b-4369-ff97-884719b2f2ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import choice\n",
        "from collections import OrderedDict\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Input, Flatten, Concatenate, Activation, Lambda\n",
        "from keras.layers import BatchNormalization as BatchNorm\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.optimizers import SGD, Adam\n",
        "# from keras.layers import LSTM\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407ed108",
      "metadata": {
        "id": "407ed108",
        "outputId": "34148acd-4956-4b17-c263-b191a4a31db7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((520, 24, 2584, 2), (130, 24, 2584, 2), (520,), (130,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "with open('sample_data/MS_mfcc_normedz_44100.pkl', 'rb') as file:\n",
        "    X = pickle.load(file)\n",
        "with open('sample_data/L_array_44100.pkl', 'rb') as file:\n",
        "    labels = pickle.load(file)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=74)\n",
        "# X = np.transpose(X, (0,2,3,1))\n",
        "X_train = np.transpose(X_train, (0,2,3,1))\n",
        "X_test = np.transpose(X_test, (0,2,3,1))\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848fa095",
      "metadata": {
        "id": "848fa095",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79bea688-b2da-4244-f316-d8d08927f487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45bfb4d",
      "metadata": {
        "id": "b45bfb4d"
      },
      "source": [
        "# Convolutional network definition\n",
        "#### Notably, the output is not a classifier and instead a numeric value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a668781",
      "metadata": {
        "id": "7a668781",
        "outputId": "9a0fe462-8a0b-4dec-96ab-bc404cb38826",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 24, 2584, 2)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 24, 2584, 64)      1600      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 2584, 64)      49216     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 24, 2584, 64)      0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 24, 2584, 64)     256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 24, 2584, 128)     98432     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 22, 2581, 128)     196736    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 22, 2581, 128)     0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 22, 2581, 128)    512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 11, 1290, 128)    0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 9, 1287, 256)      393472    \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 7, 1284, 256)      786688    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 7, 1284, 256)      0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 7, 1284, 256)     1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 7, 1284, 512)      1573376   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 7, 1284, 512)     2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 3, 642, 512)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 986112)            0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 986112)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               252444928 \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,567,137\n",
            "Trainable params: 255,564,545\n",
            "Non-trainable params: 2,592\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "### 2D SPECTROGRAM PARALLEL CONVOLUTION MODEL\n",
        "\n",
        "def create_model_2D_MFCC(resize_shape):\n",
        "    # resize_shape = 24, 2584, 2\n",
        "    # factors are 2,2,2,3 and 2,2,2,17,19\n",
        "    # Mid and side inputs\n",
        "    input_MS = Input(shape = resize_shape)\n",
        "\n",
        "    # Convolutional layers for sequence 1\n",
        "    conv = Conv2D(64, (3, 4), activation='relu', padding='same', kernel_initializer='he_normal')(input_MS)\n",
        "    conv = Conv2D(64, (3, 4), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "    conv = BatchNorm()(conv)\n",
        "    conv = Conv2D(128, (3, 4), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = Conv2D(128, (3, 4), activation='relu', kernel_initializer='he_normal')(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "    conv = BatchNorm()(conv)\n",
        "    conv = MaxPooling2D(2, 2)(conv)\n",
        "    conv = Conv2D(256, (3, 4), activation='relu', kernel_initializer='he_normal')(conv)\n",
        "    conv = Conv2D(256, (3, 4), activation='relu', kernel_initializer='he_normal')(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "    conv = BatchNorm()(conv)\n",
        "    conv = Conv2D(512, (3, 4), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
        "    conv = BatchNorm()(conv)\n",
        "    conv = MaxPooling2D(2, 2)(conv)\n",
        "    conv = Flatten()(conv)\n",
        "    conv = Dropout(0.3)(conv)\n",
        "\n",
        "    # Fully connected -> output\n",
        "    combined = Dense(256, activation='relu')(conv)\n",
        "    combined = BatchNorm()(combined)\n",
        "    combined = Dense(64)(combined)\n",
        "    combined = Dropout(0.3)(combined)\n",
        "    combined = BatchNorm()(combined)\n",
        "    combined = Dense(16, activation='relu')(combined)\n",
        "    combined = BatchNorm()(combined)\n",
        "    prediction = Dense(1, activation='sigmoid')(combined)\n",
        "\n",
        "    # Create the model with two inputs and one output\n",
        "    model = Model(inputs=input_MS, outputs=prediction)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "# Shape is one (24, 2584) or one 30 sec window split into 200 windows\n",
        "# factors are 2,2,2,3 and 2,2,2,17,19\n",
        "model_2D_spec = create_model_2D_MFCC((24, 2584, 2))\n",
        "\n",
        "# Compile the model for regression\n",
        "opt = Adam(learning_rate=0.01)\n",
        "model_2D_spec.compile(optimizer=opt, loss='mean_absolute_error', metrics=['mean_absolute_error'])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model_2D_spec.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0eaf27f",
      "metadata": {
        "id": "d0eaf27f"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abeffed7",
      "metadata": {
        "id": "abeffed7",
        "outputId": "e9b631ac-df1b-461c-be7e-049bd7943723",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.2051 - mean_absolute_error: 0.2051\n",
            "Epoch 1: val_loss improved from inf to 0.36454, saving model to 2DSpec_wBest.hdf5\n",
            "130/130 [==============================] - 61s 397ms/step - loss: 0.2051 - mean_absolute_error: 0.2051 - val_loss: 0.3645 - val_mean_absolute_error: 0.3645\n",
            "Epoch 2/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1778 - mean_absolute_error: 0.1778\n",
            "Epoch 2: val_loss improved from 0.36454 to 0.19669, saving model to 2DSpec_wBest.hdf5\n",
            "130/130 [==============================] - 52s 404ms/step - loss: 0.1778 - mean_absolute_error: 0.1778 - val_loss: 0.1967 - val_mean_absolute_error: 0.1967\n",
            "Epoch 3/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1805 - mean_absolute_error: 0.1805\n",
            "Epoch 3: val_loss did not improve from 0.19669\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1805 - mean_absolute_error: 0.1805 - val_loss: 0.3055 - val_mean_absolute_error: 0.3055\n",
            "Epoch 4/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1809 - mean_absolute_error: 0.1809\n",
            "Epoch 4: val_loss improved from 0.19669 to 0.17968, saving model to 2DSpec_wBest.hdf5\n",
            "130/130 [==============================] - 58s 452ms/step - loss: 0.1809 - mean_absolute_error: 0.1809 - val_loss: 0.1797 - val_mean_absolute_error: 0.1797\n",
            "Epoch 5/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1697 - mean_absolute_error: 0.1697\n",
            "Epoch 5: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1697 - mean_absolute_error: 0.1697 - val_loss: 0.3376 - val_mean_absolute_error: 0.3376\n",
            "Epoch 6/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1717 - mean_absolute_error: 0.1717\n",
            "Epoch 6: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1717 - mean_absolute_error: 0.1717 - val_loss: 0.1933 - val_mean_absolute_error: 0.1933\n",
            "Epoch 7/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1673 - mean_absolute_error: 0.1673\n",
            "Epoch 7: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1673 - mean_absolute_error: 0.1673 - val_loss: 0.3384 - val_mean_absolute_error: 0.3384\n",
            "Epoch 8/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1590 - mean_absolute_error: 0.1590\n",
            "Epoch 8: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1590 - mean_absolute_error: 0.1590 - val_loss: 0.1951 - val_mean_absolute_error: 0.1951\n",
            "Epoch 9/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1665 - mean_absolute_error: 0.1665\n",
            "Epoch 9: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1665 - mean_absolute_error: 0.1665 - val_loss: 0.3075 - val_mean_absolute_error: 0.3075\n",
            "Epoch 10/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1621 - mean_absolute_error: 0.1621\n",
            "Epoch 10: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1621 - mean_absolute_error: 0.1621 - val_loss: 0.3672 - val_mean_absolute_error: 0.3672\n",
            "Epoch 11/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1621 - mean_absolute_error: 0.1621\n",
            "Epoch 11: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1621 - mean_absolute_error: 0.1621 - val_loss: 0.3677 - val_mean_absolute_error: 0.3677\n",
            "Epoch 12/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1566 - mean_absolute_error: 0.1566\n",
            "Epoch 12: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1566 - mean_absolute_error: 0.1566 - val_loss: 0.1881 - val_mean_absolute_error: 0.1881\n",
            "Epoch 13/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1606 - mean_absolute_error: 0.1606\n",
            "Epoch 13: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1606 - mean_absolute_error: 0.1606 - val_loss: 0.2224 - val_mean_absolute_error: 0.2224\n",
            "Epoch 14/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1510 - mean_absolute_error: 0.1510\n",
            "Epoch 14: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1510 - mean_absolute_error: 0.1510 - val_loss: 0.3686 - val_mean_absolute_error: 0.3686\n",
            "Epoch 15/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1580 - mean_absolute_error: 0.1580\n",
            "Epoch 15: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1580 - mean_absolute_error: 0.1580 - val_loss: 0.2822 - val_mean_absolute_error: 0.2822\n",
            "Epoch 16/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1562 - mean_absolute_error: 0.1562\n",
            "Epoch 16: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1562 - mean_absolute_error: 0.1562 - val_loss: 0.1890 - val_mean_absolute_error: 0.1890\n",
            "Epoch 17/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1599 - mean_absolute_error: 0.1599\n",
            "Epoch 17: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1599 - mean_absolute_error: 0.1599 - val_loss: 0.3862 - val_mean_absolute_error: 0.3862\n",
            "Epoch 18/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1588 - mean_absolute_error: 0.1588\n",
            "Epoch 18: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1588 - mean_absolute_error: 0.1588 - val_loss: 0.2784 - val_mean_absolute_error: 0.2784\n",
            "Epoch 19/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1488 - mean_absolute_error: 0.1488\n",
            "Epoch 19: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1488 - mean_absolute_error: 0.1488 - val_loss: 0.5612 - val_mean_absolute_error: 0.5612\n",
            "Epoch 20/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1513 - mean_absolute_error: 0.1513\n",
            "Epoch 20: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1513 - mean_absolute_error: 0.1513 - val_loss: 0.2863 - val_mean_absolute_error: 0.2863\n",
            "Epoch 21/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1541 - mean_absolute_error: 0.1541\n",
            "Epoch 21: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1541 - mean_absolute_error: 0.1541 - val_loss: 0.2659 - val_mean_absolute_error: 0.2659\n",
            "Epoch 22/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1590 - mean_absolute_error: 0.1590\n",
            "Epoch 22: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 148ms/step - loss: 0.1590 - mean_absolute_error: 0.1590 - val_loss: 0.4173 - val_mean_absolute_error: 0.4173\n",
            "Epoch 23/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1605 - mean_absolute_error: 0.1605\n",
            "Epoch 23: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1605 - mean_absolute_error: 0.1605 - val_loss: 0.4114 - val_mean_absolute_error: 0.4114\n",
            "Epoch 24/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1725 - mean_absolute_error: 0.1725\n",
            "Epoch 24: val_loss did not improve from 0.17968\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1725 - mean_absolute_error: 0.1725 - val_loss: 0.1807 - val_mean_absolute_error: 0.1807\n",
            "Epoch 25/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1693 - mean_absolute_error: 0.1693\n",
            "Epoch 25: val_loss improved from 0.17968 to 0.16638, saving model to 2DSpec_wBest.hdf5\n",
            "130/130 [==============================] - 38s 295ms/step - loss: 0.1693 - mean_absolute_error: 0.1693 - val_loss: 0.1664 - val_mean_absolute_error: 0.1664\n",
            "Epoch 26/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1651 - mean_absolute_error: 0.1651\n",
            "Epoch 26: val_loss did not improve from 0.16638\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1651 - mean_absolute_error: 0.1651 - val_loss: 0.1756 - val_mean_absolute_error: 0.1756\n",
            "Epoch 27/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1691 - mean_absolute_error: 0.1691\n",
            "Epoch 27: val_loss did not improve from 0.16638\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1691 - mean_absolute_error: 0.1691 - val_loss: 0.4061 - val_mean_absolute_error: 0.4061\n",
            "Epoch 28/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1607 - mean_absolute_error: 0.1607\n",
            "Epoch 28: val_loss improved from 0.16638 to 0.16424, saving model to 2DSpec_wBest.hdf5\n",
            "130/130 [==============================] - 66s 511ms/step - loss: 0.1607 - mean_absolute_error: 0.1607 - val_loss: 0.1642 - val_mean_absolute_error: 0.1642\n",
            "Epoch 29/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1620 - mean_absolute_error: 0.1620\n",
            "Epoch 29: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1620 - mean_absolute_error: 0.1620 - val_loss: 0.1780 - val_mean_absolute_error: 0.1780\n",
            "Epoch 30/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1607 - mean_absolute_error: 0.1607\n",
            "Epoch 30: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1607 - mean_absolute_error: 0.1607 - val_loss: 0.1768 - val_mean_absolute_error: 0.1768\n",
            "Epoch 31/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1626 - mean_absolute_error: 0.1626\n",
            "Epoch 31: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1626 - mean_absolute_error: 0.1626 - val_loss: 0.1818 - val_mean_absolute_error: 0.1818\n",
            "Epoch 32/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1724 - mean_absolute_error: 0.1724\n",
            "Epoch 32: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1724 - mean_absolute_error: 0.1724 - val_loss: 0.1787 - val_mean_absolute_error: 0.1787\n",
            "Epoch 33/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1682 - mean_absolute_error: 0.1682\n",
            "Epoch 33: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1682 - mean_absolute_error: 0.1682 - val_loss: 0.1696 - val_mean_absolute_error: 0.1696\n",
            "Epoch 34/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1675 - mean_absolute_error: 0.1675\n",
            "Epoch 34: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1675 - mean_absolute_error: 0.1675 - val_loss: 0.4648 - val_mean_absolute_error: 0.4648\n",
            "Epoch 35/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1694 - mean_absolute_error: 0.1694\n",
            "Epoch 35: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1694 - mean_absolute_error: 0.1694 - val_loss: 0.1814 - val_mean_absolute_error: 0.1814\n",
            "Epoch 36/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1729 - mean_absolute_error: 0.1729\n",
            "Epoch 36: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1729 - mean_absolute_error: 0.1729 - val_loss: 0.2830 - val_mean_absolute_error: 0.2830\n",
            "Epoch 37/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1595 - mean_absolute_error: 0.1595\n",
            "Epoch 37: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1595 - mean_absolute_error: 0.1595 - val_loss: 0.1841 - val_mean_absolute_error: 0.1841\n",
            "Epoch 38/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1582 - mean_absolute_error: 0.1582\n",
            "Epoch 38: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1582 - mean_absolute_error: 0.1582 - val_loss: 0.2925 - val_mean_absolute_error: 0.2925\n",
            "Epoch 39/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1513 - mean_absolute_error: 0.1513\n",
            "Epoch 39: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1513 - mean_absolute_error: 0.1513 - val_loss: 0.1944 - val_mean_absolute_error: 0.1944\n",
            "Epoch 40/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1566 - mean_absolute_error: 0.1566\n",
            "Epoch 40: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1566 - mean_absolute_error: 0.1566 - val_loss: 0.1676 - val_mean_absolute_error: 0.1676\n",
            "Epoch 41/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1525 - mean_absolute_error: 0.1525\n",
            "Epoch 41: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1525 - mean_absolute_error: 0.1525 - val_loss: 0.1775 - val_mean_absolute_error: 0.1775\n",
            "Epoch 42/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1542 - mean_absolute_error: 0.1542\n",
            "Epoch 42: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1542 - mean_absolute_error: 0.1542 - val_loss: 0.1709 - val_mean_absolute_error: 0.1709\n",
            "Epoch 43/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1505 - mean_absolute_error: 0.1505\n",
            "Epoch 43: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1505 - mean_absolute_error: 0.1505 - val_loss: 0.1687 - val_mean_absolute_error: 0.1687\n",
            "Epoch 44/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1435 - mean_absolute_error: 0.1435\n",
            "Epoch 44: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1435 - mean_absolute_error: 0.1435 - val_loss: 0.2003 - val_mean_absolute_error: 0.2003\n",
            "Epoch 45/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1420 - mean_absolute_error: 0.1420\n",
            "Epoch 45: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1420 - mean_absolute_error: 0.1420 - val_loss: 0.1737 - val_mean_absolute_error: 0.1737\n",
            "Epoch 46/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1614 - mean_absolute_error: 0.1614\n",
            "Epoch 46: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1614 - mean_absolute_error: 0.1614 - val_loss: 0.1716 - val_mean_absolute_error: 0.1716\n",
            "Epoch 47/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1572 - mean_absolute_error: 0.1572\n",
            "Epoch 47: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1572 - mean_absolute_error: 0.1572 - val_loss: 0.1743 - val_mean_absolute_error: 0.1743\n",
            "Epoch 48/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1490 - mean_absolute_error: 0.1490\n",
            "Epoch 48: val_loss did not improve from 0.16424\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1490 - mean_absolute_error: 0.1490 - val_loss: 0.1836 - val_mean_absolute_error: 0.1836\n",
            "Epoch 49/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1333 - mean_absolute_error: 0.1333\n",
            "Epoch 49: val_loss improved from 0.16424 to 0.15761, saving model to 2DSpec_wBest.hdf5\n",
            "130/130 [==============================] - 59s 459ms/step - loss: 0.1333 - mean_absolute_error: 0.1333 - val_loss: 0.1576 - val_mean_absolute_error: 0.1576\n",
            "Epoch 50/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1377 - mean_absolute_error: 0.1377\n",
            "Epoch 50: val_loss did not improve from 0.15761\n",
            "130/130 [==============================] - 19s 148ms/step - loss: 0.1377 - mean_absolute_error: 0.1377 - val_loss: 0.1682 - val_mean_absolute_error: 0.1682\n",
            "Epoch 51/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1321 - mean_absolute_error: 0.1321\n",
            "Epoch 51: val_loss did not improve from 0.15761\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1321 - mean_absolute_error: 0.1321 - val_loss: 0.1821 - val_mean_absolute_error: 0.1821\n",
            "Epoch 52/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1455 - mean_absolute_error: 0.1455\n",
            "Epoch 52: val_loss did not improve from 0.15761\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1455 - mean_absolute_error: 0.1455 - val_loss: 0.1942 - val_mean_absolute_error: 0.1942\n",
            "Epoch 53/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1377 - mean_absolute_error: 0.1377\n",
            "Epoch 53: val_loss did not improve from 0.15761\n",
            "130/130 [==============================] - 19s 148ms/step - loss: 0.1377 - mean_absolute_error: 0.1377 - val_loss: 0.1583 - val_mean_absolute_error: 0.1583\n",
            "Epoch 54/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1313 - mean_absolute_error: 0.1313\n",
            "Epoch 54: val_loss improved from 0.15761 to 0.15640, saving model to 2DSpec_wBest.hdf5\n",
            "130/130 [==============================] - 78s 603ms/step - loss: 0.1313 - mean_absolute_error: 0.1313 - val_loss: 0.1564 - val_mean_absolute_error: 0.1564\n",
            "Epoch 55/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1367 - mean_absolute_error: 0.1367\n",
            "Epoch 55: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1367 - mean_absolute_error: 0.1367 - val_loss: 0.1857 - val_mean_absolute_error: 0.1857\n",
            "Epoch 56/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1336 - mean_absolute_error: 0.1336\n",
            "Epoch 56: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1336 - mean_absolute_error: 0.1336 - val_loss: 0.2079 - val_mean_absolute_error: 0.2079\n",
            "Epoch 57/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1553 - mean_absolute_error: 0.1553\n",
            "Epoch 57: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1553 - mean_absolute_error: 0.1553 - val_loss: 0.1685 - val_mean_absolute_error: 0.1685\n",
            "Epoch 58/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1622 - mean_absolute_error: 0.1622\n",
            "Epoch 58: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1622 - mean_absolute_error: 0.1622 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
            "Epoch 59/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1599 - mean_absolute_error: 0.1599\n",
            "Epoch 59: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1599 - mean_absolute_error: 0.1599 - val_loss: 0.1678 - val_mean_absolute_error: 0.1678\n",
            "Epoch 60/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1466 - mean_absolute_error: 0.1466\n",
            "Epoch 60: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1466 - mean_absolute_error: 0.1466 - val_loss: 0.1839 - val_mean_absolute_error: 0.1839\n",
            "Epoch 61/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1475 - mean_absolute_error: 0.1475\n",
            "Epoch 61: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1475 - mean_absolute_error: 0.1475 - val_loss: 0.2287 - val_mean_absolute_error: 0.2287\n",
            "Epoch 62/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1491 - mean_absolute_error: 0.1491\n",
            "Epoch 62: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1491 - mean_absolute_error: 0.1491 - val_loss: 0.1661 - val_mean_absolute_error: 0.1661\n",
            "Epoch 63/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1516 - mean_absolute_error: 0.1516\n",
            "Epoch 63: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1516 - mean_absolute_error: 0.1516 - val_loss: 0.2729 - val_mean_absolute_error: 0.2729\n",
            "Epoch 64/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1414 - mean_absolute_error: 0.1414\n",
            "Epoch 64: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1414 - mean_absolute_error: 0.1414 - val_loss: 0.4327 - val_mean_absolute_error: 0.4327\n",
            "Epoch 65/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1405 - mean_absolute_error: 0.1405\n",
            "Epoch 65: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1405 - mean_absolute_error: 0.1405 - val_loss: 0.5679 - val_mean_absolute_error: 0.5679\n",
            "Epoch 66/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1353 - mean_absolute_error: 0.1353\n",
            "Epoch 66: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1353 - mean_absolute_error: 0.1353 - val_loss: 0.3245 - val_mean_absolute_error: 0.3245\n",
            "Epoch 67/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1378 - mean_absolute_error: 0.1378\n",
            "Epoch 67: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1378 - mean_absolute_error: 0.1378 - val_loss: 0.2921 - val_mean_absolute_error: 0.2921\n",
            "Epoch 68/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1371 - mean_absolute_error: 0.1371\n",
            "Epoch 68: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1371 - mean_absolute_error: 0.1371 - val_loss: 0.3161 - val_mean_absolute_error: 0.3161\n",
            "Epoch 69/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1419 - mean_absolute_error: 0.1419\n",
            "Epoch 69: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1419 - mean_absolute_error: 0.1419 - val_loss: 0.4159 - val_mean_absolute_error: 0.4159\n",
            "Epoch 70/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1275 - mean_absolute_error: 0.1275\n",
            "Epoch 70: val_loss did not improve from 0.15640\n",
            "130/130 [==============================] - 19s 143ms/step - loss: 0.1275 - mean_absolute_error: 0.1275 - val_loss: 0.2562 - val_mean_absolute_error: 0.2562\n",
            "Epoch 71/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1329 - mean_absolute_error: 0.1329\n",
            "Epoch 71: val_loss improved from 0.15640 to 0.15403, saving model to 2DSpec_wBest.hdf5\n",
            "130/130 [==============================] - 65s 502ms/step - loss: 0.1329 - mean_absolute_error: 0.1329 - val_loss: 0.1540 - val_mean_absolute_error: 0.1540\n",
            "Epoch 72/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1325 - mean_absolute_error: 0.1325\n",
            "Epoch 72: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1325 - mean_absolute_error: 0.1325 - val_loss: 0.4343 - val_mean_absolute_error: 0.4343\n",
            "Epoch 73/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1464 - mean_absolute_error: 0.1464\n",
            "Epoch 73: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1464 - mean_absolute_error: 0.1464 - val_loss: 0.5748 - val_mean_absolute_error: 0.5748\n",
            "Epoch 74/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1562 - mean_absolute_error: 0.1562\n",
            "Epoch 74: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1562 - mean_absolute_error: 0.1562 - val_loss: 0.2386 - val_mean_absolute_error: 0.2386\n",
            "Epoch 75/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1490 - mean_absolute_error: 0.1490\n",
            "Epoch 75: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1490 - mean_absolute_error: 0.1490 - val_loss: 0.1771 - val_mean_absolute_error: 0.1771\n",
            "Epoch 76/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1461 - mean_absolute_error: 0.1461\n",
            "Epoch 76: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1461 - mean_absolute_error: 0.1461 - val_loss: 0.1887 - val_mean_absolute_error: 0.1887\n",
            "Epoch 77/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1457 - mean_absolute_error: 0.1457\n",
            "Epoch 77: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1457 - mean_absolute_error: 0.1457 - val_loss: 0.2984 - val_mean_absolute_error: 0.2984\n",
            "Epoch 78/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1328 - mean_absolute_error: 0.1328\n",
            "Epoch 78: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1328 - mean_absolute_error: 0.1328 - val_loss: 0.2672 - val_mean_absolute_error: 0.2672\n",
            "Epoch 79/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1313 - mean_absolute_error: 0.1313\n",
            "Epoch 79: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1313 - mean_absolute_error: 0.1313 - val_loss: 0.2192 - val_mean_absolute_error: 0.2192\n",
            "Epoch 80/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1309 - mean_absolute_error: 0.1309\n",
            "Epoch 80: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1309 - mean_absolute_error: 0.1309 - val_loss: 0.2072 - val_mean_absolute_error: 0.2072\n",
            "Epoch 81/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1330 - mean_absolute_error: 0.1330\n",
            "Epoch 81: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1330 - mean_absolute_error: 0.1330 - val_loss: 0.1937 - val_mean_absolute_error: 0.1937\n",
            "Epoch 82/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1269 - mean_absolute_error: 0.1269\n",
            "Epoch 82: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1269 - mean_absolute_error: 0.1269 - val_loss: 0.1704 - val_mean_absolute_error: 0.1704\n",
            "Epoch 83/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1227 - mean_absolute_error: 0.1227\n",
            "Epoch 83: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1227 - mean_absolute_error: 0.1227 - val_loss: 0.1989 - val_mean_absolute_error: 0.1989\n",
            "Epoch 84/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1236 - mean_absolute_error: 0.1236\n",
            "Epoch 84: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1236 - mean_absolute_error: 0.1236 - val_loss: 0.1911 - val_mean_absolute_error: 0.1911\n",
            "Epoch 85/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1214 - mean_absolute_error: 0.1214\n",
            "Epoch 85: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1687 - val_mean_absolute_error: 0.1687\n",
            "Epoch 86/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1167 - mean_absolute_error: 0.1167\n",
            "Epoch 86: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1167 - mean_absolute_error: 0.1167 - val_loss: 0.2084 - val_mean_absolute_error: 0.2084\n",
            "Epoch 87/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1297 - mean_absolute_error: 0.1297\n",
            "Epoch 87: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1297 - mean_absolute_error: 0.1297 - val_loss: 0.1652 - val_mean_absolute_error: 0.1652\n",
            "Epoch 88/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1290 - mean_absolute_error: 0.1290\n",
            "Epoch 88: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1290 - mean_absolute_error: 0.1290 - val_loss: 0.1727 - val_mean_absolute_error: 0.1727\n",
            "Epoch 89/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1280 - mean_absolute_error: 0.1280\n",
            "Epoch 89: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1280 - mean_absolute_error: 0.1280 - val_loss: 0.1859 - val_mean_absolute_error: 0.1859\n",
            "Epoch 90/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1214 - mean_absolute_error: 0.1214\n",
            "Epoch 90: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1916 - val_mean_absolute_error: 0.1916\n",
            "Epoch 91/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1245 - mean_absolute_error: 0.1245\n",
            "Epoch 91: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1245 - mean_absolute_error: 0.1245 - val_loss: 0.1834 - val_mean_absolute_error: 0.1834\n",
            "Epoch 92/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1183 - mean_absolute_error: 0.1183\n",
            "Epoch 92: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1183 - mean_absolute_error: 0.1183 - val_loss: 0.2334 - val_mean_absolute_error: 0.2334\n",
            "Epoch 93/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1222 - mean_absolute_error: 0.1222\n",
            "Epoch 93: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.1672 - val_mean_absolute_error: 0.1672\n",
            "Epoch 94/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1118 - mean_absolute_error: 0.1118\n",
            "Epoch 94: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1118 - mean_absolute_error: 0.1118 - val_loss: 0.1588 - val_mean_absolute_error: 0.1588\n",
            "Epoch 95/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1245 - mean_absolute_error: 0.1245\n",
            "Epoch 95: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1245 - mean_absolute_error: 0.1245 - val_loss: 0.1899 - val_mean_absolute_error: 0.1899\n",
            "Epoch 96/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1196 - mean_absolute_error: 0.1196\n",
            "Epoch 96: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1196 - mean_absolute_error: 0.1196 - val_loss: 0.1995 - val_mean_absolute_error: 0.1995\n",
            "Epoch 97/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1218 - mean_absolute_error: 0.1218\n",
            "Epoch 97: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1627 - val_mean_absolute_error: 0.1627\n",
            "Epoch 98/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1299 - mean_absolute_error: 0.1299\n",
            "Epoch 98: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1299 - mean_absolute_error: 0.1299 - val_loss: 0.1820 - val_mean_absolute_error: 0.1820\n",
            "Epoch 99/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1260 - mean_absolute_error: 0.1260\n",
            "Epoch 99: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1260 - mean_absolute_error: 0.1260 - val_loss: 0.1812 - val_mean_absolute_error: 0.1812\n",
            "Epoch 100/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1149 - mean_absolute_error: 0.1149\n",
            "Epoch 100: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1149 - mean_absolute_error: 0.1149 - val_loss: 0.1709 - val_mean_absolute_error: 0.1709\n",
            "Epoch 101/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1238 - mean_absolute_error: 0.1238\n",
            "Epoch 101: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1238 - mean_absolute_error: 0.1238 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "Epoch 102/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1216 - mean_absolute_error: 0.1216\n",
            "Epoch 102: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.2039 - val_mean_absolute_error: 0.2039\n",
            "Epoch 103/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1186 - mean_absolute_error: 0.1186\n",
            "Epoch 103: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1186 - mean_absolute_error: 0.1186 - val_loss: 0.1863 - val_mean_absolute_error: 0.1863\n",
            "Epoch 104/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1161 - mean_absolute_error: 0.1161\n",
            "Epoch 104: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 144ms/step - loss: 0.1161 - mean_absolute_error: 0.1161 - val_loss: 0.2008 - val_mean_absolute_error: 0.2008\n",
            "Epoch 105/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1186 - mean_absolute_error: 0.1186\n",
            "Epoch 105: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 143ms/step - loss: 0.1186 - mean_absolute_error: 0.1186 - val_loss: 0.1647 - val_mean_absolute_error: 0.1647\n",
            "Epoch 106/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1249 - mean_absolute_error: 0.1249\n",
            "Epoch 106: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1249 - mean_absolute_error: 0.1249 - val_loss: 0.1845 - val_mean_absolute_error: 0.1845\n",
            "Epoch 107/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1134 - mean_absolute_error: 0.1134\n",
            "Epoch 107: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1134 - mean_absolute_error: 0.1134 - val_loss: 0.2045 - val_mean_absolute_error: 0.2045\n",
            "Epoch 108/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1160 - mean_absolute_error: 0.1160\n",
            "Epoch 108: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1160 - mean_absolute_error: 0.1160 - val_loss: 0.1943 - val_mean_absolute_error: 0.1943\n",
            "Epoch 109/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1145 - mean_absolute_error: 0.1145\n",
            "Epoch 109: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1145 - mean_absolute_error: 0.1145 - val_loss: 0.1701 - val_mean_absolute_error: 0.1701\n",
            "Epoch 110/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1233 - mean_absolute_error: 0.1233\n",
            "Epoch 110: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1233 - mean_absolute_error: 0.1233 - val_loss: 0.1895 - val_mean_absolute_error: 0.1895\n",
            "Epoch 111/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1222 - mean_absolute_error: 0.1222\n",
            "Epoch 111: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.2246 - val_mean_absolute_error: 0.2246\n",
            "Epoch 112/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1241 - mean_absolute_error: 0.1241\n",
            "Epoch 112: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1241 - mean_absolute_error: 0.1241 - val_loss: 0.1919 - val_mean_absolute_error: 0.1919\n",
            "Epoch 113/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1232 - mean_absolute_error: 0.1232\n",
            "Epoch 113: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1232 - mean_absolute_error: 0.1232 - val_loss: 0.2119 - val_mean_absolute_error: 0.2119\n",
            "Epoch 114/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1285 - mean_absolute_error: 0.1285\n",
            "Epoch 114: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1285 - mean_absolute_error: 0.1285 - val_loss: 0.1674 - val_mean_absolute_error: 0.1674\n",
            "Epoch 115/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1280 - mean_absolute_error: 0.1280\n",
            "Epoch 115: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1280 - mean_absolute_error: 0.1280 - val_loss: 0.1638 - val_mean_absolute_error: 0.1638\n",
            "Epoch 116/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1280 - mean_absolute_error: 0.1280\n",
            "Epoch 116: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1280 - mean_absolute_error: 0.1280 - val_loss: 0.1831 - val_mean_absolute_error: 0.1831\n",
            "Epoch 117/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1211 - mean_absolute_error: 0.1211\n",
            "Epoch 117: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1648 - val_mean_absolute_error: 0.1648\n",
            "Epoch 118/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1211 - mean_absolute_error: 0.1211\n",
            "Epoch 118: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1726 - val_mean_absolute_error: 0.1726\n",
            "Epoch 119/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1266 - mean_absolute_error: 0.1266\n",
            "Epoch 119: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1266 - mean_absolute_error: 0.1266 - val_loss: 0.1852 - val_mean_absolute_error: 0.1852\n",
            "Epoch 120/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1256 - mean_absolute_error: 0.1256\n",
            "Epoch 120: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1256 - mean_absolute_error: 0.1256 - val_loss: 0.1901 - val_mean_absolute_error: 0.1901\n",
            "Epoch 121/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1250 - mean_absolute_error: 0.1250\n",
            "Epoch 121: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1250 - mean_absolute_error: 0.1250 - val_loss: 0.1895 - val_mean_absolute_error: 0.1895\n",
            "Epoch 122/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1168 - mean_absolute_error: 0.1168\n",
            "Epoch 122: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1168 - mean_absolute_error: 0.1168 - val_loss: 0.1792 - val_mean_absolute_error: 0.1792\n",
            "Epoch 123/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1245 - mean_absolute_error: 0.1245\n",
            "Epoch 123: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1245 - mean_absolute_error: 0.1245 - val_loss: 0.2057 - val_mean_absolute_error: 0.2057\n",
            "Epoch 124/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1242 - mean_absolute_error: 0.1242\n",
            "Epoch 124: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1242 - mean_absolute_error: 0.1242 - val_loss: 0.2041 - val_mean_absolute_error: 0.2041\n",
            "Epoch 125/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1164 - mean_absolute_error: 0.1164\n",
            "Epoch 125: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1164 - mean_absolute_error: 0.1164 - val_loss: 0.1714 - val_mean_absolute_error: 0.1714\n",
            "Epoch 126/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1154 - mean_absolute_error: 0.1154\n",
            "Epoch 126: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1154 - mean_absolute_error: 0.1154 - val_loss: 0.1959 - val_mean_absolute_error: 0.1959\n",
            "Epoch 127/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1197 - mean_absolute_error: 0.1197\n",
            "Epoch 127: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1197 - mean_absolute_error: 0.1197 - val_loss: 0.1586 - val_mean_absolute_error: 0.1586\n",
            "Epoch 128/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1179 - mean_absolute_error: 0.1179\n",
            "Epoch 128: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1179 - mean_absolute_error: 0.1179 - val_loss: 0.2337 - val_mean_absolute_error: 0.2337\n",
            "Epoch 129/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1136 - mean_absolute_error: 0.1136\n",
            "Epoch 129: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1136 - mean_absolute_error: 0.1136 - val_loss: 0.1712 - val_mean_absolute_error: 0.1712\n",
            "Epoch 130/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1138 - mean_absolute_error: 0.1138\n",
            "Epoch 130: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1138 - mean_absolute_error: 0.1138 - val_loss: 0.1737 - val_mean_absolute_error: 0.1737\n",
            "Epoch 131/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1145 - mean_absolute_error: 0.1145\n",
            "Epoch 131: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1145 - mean_absolute_error: 0.1145 - val_loss: 0.1717 - val_mean_absolute_error: 0.1717\n",
            "Epoch 132/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1169 - mean_absolute_error: 0.1169\n",
            "Epoch 132: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1169 - mean_absolute_error: 0.1169 - val_loss: 0.2076 - val_mean_absolute_error: 0.2076\n",
            "Epoch 133/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1136 - mean_absolute_error: 0.1136\n",
            "Epoch 133: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1136 - mean_absolute_error: 0.1136 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
            "Epoch 134/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1046 - mean_absolute_error: 0.1046\n",
            "Epoch 134: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1046 - mean_absolute_error: 0.1046 - val_loss: 0.1725 - val_mean_absolute_error: 0.1725\n",
            "Epoch 135/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1179 - mean_absolute_error: 0.1179\n",
            "Epoch 135: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1179 - mean_absolute_error: 0.1179 - val_loss: 0.1991 - val_mean_absolute_error: 0.1991\n",
            "Epoch 136/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1064 - mean_absolute_error: 0.1064\n",
            "Epoch 136: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1064 - mean_absolute_error: 0.1064 - val_loss: 0.1949 - val_mean_absolute_error: 0.1949\n",
            "Epoch 137/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1113 - mean_absolute_error: 0.1113\n",
            "Epoch 137: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1113 - mean_absolute_error: 0.1113 - val_loss: 0.1742 - val_mean_absolute_error: 0.1742\n",
            "Epoch 138/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1158 - mean_absolute_error: 0.1158\n",
            "Epoch 138: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1158 - mean_absolute_error: 0.1158 - val_loss: 0.1741 - val_mean_absolute_error: 0.1741\n",
            "Epoch 139/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1090 - mean_absolute_error: 0.1090\n",
            "Epoch 139: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1090 - mean_absolute_error: 0.1090 - val_loss: 0.1550 - val_mean_absolute_error: 0.1550\n",
            "Epoch 140/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1156 - mean_absolute_error: 0.1156\n",
            "Epoch 140: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1156 - mean_absolute_error: 0.1156 - val_loss: 0.1752 - val_mean_absolute_error: 0.1752\n",
            "Epoch 141/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1133 - mean_absolute_error: 0.1133\n",
            "Epoch 141: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1133 - mean_absolute_error: 0.1133 - val_loss: 0.1753 - val_mean_absolute_error: 0.1753\n",
            "Epoch 142/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1092 - mean_absolute_error: 0.1092\n",
            "Epoch 142: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1092 - mean_absolute_error: 0.1092 - val_loss: 0.1654 - val_mean_absolute_error: 0.1654\n",
            "Epoch 143/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1056 - mean_absolute_error: 0.1056\n",
            "Epoch 143: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1056 - mean_absolute_error: 0.1056 - val_loss: 0.1934 - val_mean_absolute_error: 0.1934\n",
            "Epoch 144/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1167 - mean_absolute_error: 0.1167\n",
            "Epoch 144: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1167 - mean_absolute_error: 0.1167 - val_loss: 0.1626 - val_mean_absolute_error: 0.1626\n",
            "Epoch 145/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1253 - mean_absolute_error: 0.1253\n",
            "Epoch 145: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 146ms/step - loss: 0.1253 - mean_absolute_error: 0.1253 - val_loss: 0.1751 - val_mean_absolute_error: 0.1751\n",
            "Epoch 146/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1203 - mean_absolute_error: 0.1203\n",
            "Epoch 146: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1203 - mean_absolute_error: 0.1203 - val_loss: 0.1895 - val_mean_absolute_error: 0.1895\n",
            "Epoch 147/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1150 - mean_absolute_error: 0.1150\n",
            "Epoch 147: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 147ms/step - loss: 0.1150 - mean_absolute_error: 0.1150 - val_loss: 0.1624 - val_mean_absolute_error: 0.1624\n",
            "Epoch 148/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1057 - mean_absolute_error: 0.1057\n",
            "Epoch 148: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1057 - mean_absolute_error: 0.1057 - val_loss: 0.1735 - val_mean_absolute_error: 0.1735\n",
            "Epoch 149/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1100 - mean_absolute_error: 0.1100\n",
            "Epoch 149: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1100 - mean_absolute_error: 0.1100 - val_loss: 0.1835 - val_mean_absolute_error: 0.1835\n",
            "Epoch 150/150\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.1107 - mean_absolute_error: 0.1107\n",
            "Epoch 150: val_loss did not improve from 0.15403\n",
            "130/130 [==============================] - 19s 145ms/step - loss: 0.1107 - mean_absolute_error: 0.1107 - val_loss: 0.1795 - val_mean_absolute_error: 0.1795\n"
          ]
        }
      ],
      "source": [
        "# X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "# ((520, 2, 24, 2584), (130, 2, 24, 2584), (520,), (130,))\n",
        "\n",
        "def train_network(model):\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        \"2DSpec_wBest.hdf5\",\n",
        "        monitor='val_loss',\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    callbacks_list = [checkpoint]\n",
        "    # early_stopping = EarlyStopping(monitor='val_loss', patience=50, mode='min', verbose=1)\n",
        "\n",
        "    hist = model.fit(x=X_train, y=y_train, batch_size=4, epochs=150, shuffle=True, validation_data = (X_test, y_test), callbacks = callbacks_list)\n",
        "    model.save_weights('drive/MyDrive/Spotify feature classification/Code/Data and Weights/2DSpec_wCurrent.h5')\n",
        "    model.save_weights('2DSpec_wCurrent.h5')\n",
        "\n",
        "    file_path = '2DSpec_wBest.hdf5'\n",
        "    destination_path = 'drive/MyDrive/Spotify feature classification/Code/Data and Weights/'\n",
        "    shutil.copyfile(file_path, destination_path + '2DSpec_wBest.hdf5')\n",
        "\n",
        "    return hist\n",
        "\n",
        "history = train_network(model_2D_spec)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Training loss vs testing loss\")\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "4mJgb0_xJ3s8",
        "outputId": "dbcdb0e8-6c1b-4d2a-ef36-992df6816afe"
      },
      "id": "4mJgb0_xJ3s8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b67e7fab760>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serious variation in validation loss can be seen thoughout training, and while there is somewhat of a taming effect, it appears to be uncalibrated."
      ],
      "metadata": {
        "id": "ah4eScjoJqix"
      },
      "id": "ah4eScjoJqix"
    },
    {
      "cell_type": "markdown",
      "id": "799a2b62",
      "metadata": {
        "id": "799a2b62"
      },
      "source": [
        "# Test the model and plot results\n",
        "Accuracy is measured in mean error, standard deviation, and correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c6eed5",
      "metadata": {
        "id": "b3c6eed5"
      },
      "outputs": [],
      "source": [
        "model_2D_spec = create_model_2D_MFCC((24, 2584, 2))\n",
        "model_2D_spec.load_weights('drive/MyDrive/Spotify feature classification/Code/Data and Weights/2DSpec_wCurrent.h5')\n",
        "\n",
        "def correlation_coefficient(X, Y):\n",
        "    # Check if both arrays have the same length\n",
        "    if len(X) != len(Y):\n",
        "        raise ValueError(\"Both arrays must have the same length.\")\n",
        "\n",
        "    n = len(X)\n",
        "\n",
        "    # Calculate the means of both arrays\n",
        "    mean_X = np.mean(X)\n",
        "    mean_Y = np.mean(Y)\n",
        "\n",
        "    # Calculate the numerator of the correlation coefficient\n",
        "    numerator = np.sum((X - mean_X) * (Y - mean_Y))\n",
        "\n",
        "    # Calculate the denominator of the correlation coefficient\n",
        "    denominator = np.sqrt(np.sum((X - mean_X)**2) * np.sum((Y - mean_Y)**2))\n",
        "\n",
        "    # Calculate the correlation coefficient (r)\n",
        "    r = numerator / denominator\n",
        "\n",
        "    return r"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### MEAN ERRORS of train and test\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "train_pred = model_2D_spec.predict(X_train[:])\n",
        "test_pred = model_2D_spec.predict(X_test[:])\n",
        "\n",
        "train_error = np.sum(np.abs(train_pred - y_train))/len(y_train)\n",
        "test_error = np.sum(np.abs(test_pred - y_test))/len(y_test)\n",
        "print(\"Average training error (absolute):\", train_error)\n",
        "print(\"Average testing error (absolute):\", test_error)\n",
        "print()\n",
        "print(\"Standard deviation of train error:\", np.sqrt(np.sum(np.abs(np.abs(train_pred - y_train)-train_error))/len(y_train)))\n",
        "print(\"Standard deviation of test error:\", np.sqrt(np.sum(np.abs(np.abs(test_pred - y_test)-test_error))/len(y_test)))\n",
        "\n",
        "train_error = np.sum(np.square(train_pred - y_train))/len(y_train)\n",
        "test_error = np.sum(np.square(test_pred - y_test))/len(y_test)\n",
        "print(\"Average training error (square):\", train_error)\n",
        "print(\"Average testing error (square):\", test_error)\n",
        "\n",
        "print()\n",
        "### Chance to be significantly off\n",
        "print(len(y_train[np.abs(train_pred - y_train) > 0.2]) / len(y_train))\n",
        "print(len(y_test[np.abs(test_pred - y_test) > 0.2]) / len(y_test))\n",
        "print()\n",
        "result = correlation_coefficient(train_pred, y_train.reshape(-1, 1))\n",
        "print(\"Correlation coefficient train:\", result)\n",
        "result = correlation_coefficient(test_pred, y_test.reshape(-1, 1))\n",
        "print(\"Correlation coefficient test:\", result)"
      ],
      "metadata": {
        "id": "uogGkaaVTcuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b046986-b033-4d3e-b6b6-968464f56b50"
      },
      "id": "uogGkaaVTcuz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 9s 262ms/step\n",
            "5/5 [==============================] - 1s 192ms/step\n",
            "Average training error (absolute): 0.1170647562415783\n",
            "Average testing error (absolute): 0.17952480562485182\n",
            "\n",
            "Standard deviation of train error: 0.2516145736680124\n",
            "Standard deviation of test error: 0.32396264517964907\n",
            "Average training error (square): 0.020773771802807185\n",
            "Average testing error (square): 0.047514673412169806\n",
            "\n",
            "0.1576923076923077\n",
            "0.4153846153846154\n",
            "\n",
            "Correlation coefficient train: 0.8105989562934673\n",
            "Correlation coefficient test: 0.2715469331648932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As we look at the correlation of the fully fit model, we see that the testing predictions are actually significantly less correlated than the training.\n",
        "\n",
        "It seems like we have an overfitting issue.  In that case, lets try adding some dropout!  I'll also try adding a layer at 64 and 128"
      ],
      "metadata": {
        "id": "ceIBu-XrYlLa"
      },
      "id": "ceIBu-XrYlLa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's check our most accurate testing iteration"
      ],
      "metadata": {
        "id": "XxppxaJuHlaC"
      },
      "id": "XxppxaJuHlaC"
    },
    {
      "cell_type": "code",
      "source": [
        "model_2D_spec.load_weights(\"drive/MyDrive/Spotify feature classification/Code/Data and Weights/2DSpec_wBest.hdf5\")\n",
        "\n",
        "### MEAN ERRORS of train and test\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "train_pred = model_2D_spec.predict(X_train[:])\n",
        "test_pred = model_2D_spec.predict(X_test[:])\n",
        "print()\n",
        "train_error = np.sum(np.abs(train_pred - y_train))/len(y_train)\n",
        "test_error = np.sum(np.abs(test_pred - y_test))/len(y_test)\n",
        "print(\"Average training error (absolute):\", train_error)\n",
        "print(\"Average testing error (absolute):\", test_error)\n",
        "print()\n",
        "print(\"Standard deviation of train error:\", np.sqrt(np.sum(np.abs(np.abs(train_pred - y_train)-train_error))/len(y_train)))\n",
        "print(\"Standard deviation of test error:\", np.sqrt(np.sum(np.abs(np.abs(test_pred - y_test)-test_error))/len(y_test)))\n",
        "\n",
        "train_error = np.sum(np.square(train_pred - y_train))/len(y_train)\n",
        "test_error = np.sum(np.square(test_pred - y_test))/len(y_test)\n",
        "print(\"Average training error (square):\", train_error)\n",
        "print(\"Average testing error (square):\", test_error)\n",
        "\n",
        "print()\n",
        "print(len(y_train[np.abs(train_pred - y_train) > 0.2]) / len(y_train), \"% training are significantly off.\")\n",
        "print(len(y_test[np.abs(test_pred - y_test) > 0.2]) / len(y_test), \"% testing are significantly off.\")\n",
        "print()\n",
        "result = correlation_coefficient(train_pred, y_train.reshape(-1, 1))\n",
        "print(\"Correlation coefficient train:\", result)\n",
        "result = correlation_coefficient(test_pred, y_test.reshape(-1, 1))\n",
        "print(\"Correlation coefficient test:\", result)"
      ],
      "metadata": {
        "id": "Ka3ghjAs19l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b91a568b-a100-482b-d42b-e873ec619cf6"
      },
      "id": "Ka3ghjAs19l4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 3s 193ms/step\n",
            "5/5 [==============================] - 1s 190ms/step\n",
            "\n",
            "Average training error (absolute): 0.11683412769932014\n",
            "Average testing error (absolute): 0.15402635269733575\n",
            "\n",
            "Standard deviation of train error: 0.2833553159283039\n",
            "Standard deviation of test error: 0.3346633127925176\n",
            "Average training error (square): 0.023229922773968986\n",
            "Average testing error (square): 0.040930798506728164\n",
            "\n",
            "0.21346153846153845 % training are significantly off.\n",
            "0.3384615384615385 % testing are significantly off.\n",
            "\n",
            "Correlation coefficient train: 0.6833577808183664\n",
            "Correlation coefficient test: 0.3463623757422866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The train and test results are for more consistent here, but it can be seen that the effect size is also significantly reduced."
      ],
      "metadata": {
        "id": "NDd97AiGAiF8"
      },
      "id": "NDd97AiGAiF8"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_g85V0KAt4H"
      },
      "id": "K_g85V0KAt4H",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}